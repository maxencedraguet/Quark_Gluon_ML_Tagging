experiment_name:                    test                      # Experiment name.
use_gpu:                            False                     # Use a GPU if available.
seed:                               1                         # Random seed to ensure reproducibility.
dataset:                            Set4                      # Data to use (Set1, Set2, Set3, Set4...)

# Path to the dataset directory. SHOULD MATCH THE DATASET INDICATED JUST ABOVE
#   For Set 1: a directory with the CSV for test (all ttbar)
#   For Set 2/3: the complete path to the h5 file: /data/atlas/atlasdata3/mdraguet/Set[2 or 3]/HF/
#   For Junipr dataset (Set 4) example: ../example_JUNIPR_data.json
#   For Junipr dataset (Set 4): /data/atlas/atlasdata3/mdraguet/Set4/junipr/mc16_13TeV_410470_PhPy8EG_A14_ttbar_hdamp258p75_nonallhad_deriv_DAOD_JETM6.json
#   For Junipr dataset (Set 4) and tri cuts (pT, constituents and SumTrackPT500): /data/atlas/atlasdata3/mdraguet/Set4_tri_cut/junipr/mc16_13TeV_410470_PhPy8EG_A14_ttbar_hdamp258p75_nonallhad_deriv_DAOD_JETM6.json

#   For UpRootTransformer: the complete path to a text file with the list of directories to load: /home/draguet/QGdis/Code/Data/Set [2 or 3] /loader_list.txt
#   For GranularUpRootTransformer: the complete path to a text file with the list of directories to load: /home/draguet/QGdis/Code/Data/Set4/loader_list.txt
#   For Train_Test_Separator: the complete path to a h5 file to split into train-validation-test: /data/atlas/atlasdata3/mdraguet/Set2/HF/mc16aprocessed.h5
absolute_data_path:                 /home/draguet/QGdis/Code/Data/Set4/loader_list.txt
fraction_data:                      1                         # Fraction of the data to load.
test_size:                          0.6                       # Fraction of data to use for test
equilibrate_data:                   True                      # Whether to guarantee same number of quark jets as gluon jets

cross_section_sampler:                                        # A cross section sampler. Works with a file of cross section indexed by dsid: Backgrounds.txt
    cs_based:                       False                     # Whether to use the cross section sampler of the direct one
    n_samples:                      1000000                   # Number of samples demanded with the cross section sampler.
                                                              #         The final number may differ: rare processes are at least sampled once (increase number)
                                                              #         and if too many samples are asked, the demand is trimmed to what is available.

# Model to implement (BDT, NN, Junipr ...) or method to run (UpRootTransformer, GranularUpRootTransformer, Train_Test_Separator ...)
# Warning, this is case sensitive
experiment_type:                    GranularUpRootTransformer

save_model:                         True                      # Whether to save the model or not
diagnostic:                         True                      # For appropriate methods, saves some diagnostic information (e.g.: UpRootTransformer.py, GranularUpRootTransformer, ...)

NN_Model:
    lr:                             0.001
    lr_scheduler:                   True
    dropout_proba:                  0
    epoch:                          10
    batch_size:                     128
    loss_function:                  BCE_log                   # Loss function to use: BCE_log (binary-cross entropy with logit), BCE (same without the sigmoid)
    initialisation:                 xavier_normal             # Initialisation of weights: xavier_uniform or xavier_normal
    test_frequency:                 100                       # Frequency of test error estimation during traing (for plotting purpose)
    optimiser:
        type:                       adam
        params:                     [0.9, 0.999, 0.00000001]  # Parameters of the optimiser: should be general enough but could be optimised.
        weight_decay:               0.0000001                   # Weight decay for L2 regularisation
    NeuralNet:
        input_dimensions:           [14, 32, 32, 1]
        nonlinearity:               relu                      # relu, elu, tanh, sigmoid, identity
        end_nonlinearity:           identity                  # relu, elu, tanh, sigmoid, identity

BDT_model:                                                    # For AdaBoostClassifier of sklearn
    n_estim:                        300                       # Number of estimators
    base_estimator:                 DecisionTreeClassifier    # Base type of estimators
    max_depth:                      3                         # Maximal dept of the base estimator (decision tree)
    lr:                             0.15                      # Learning rate
    grid_search:                    False                     # Whether to perform grid search

Junipr_Model:
    binary_runner_bool:             False                     # If true, will load two models (as specified in binary_runner) to perform a binarised-version of Junipr
    train:                          True                      # If true, train the model
    assess:                         True                      # If true, assess the model
    load:                           False                     # If true, load a model as specified below
    load_model_path:                /home/draguet/QGdis/Code/Experiments/Results/2020-07-19-15-07-13/test/saved_JUNIPR_weights.pt
    load_model_config:              /home/draguet/QGdis/Code/Experiments/Results/2020-07-19-15-07-13/test/base_config.yaml
    binary_runner:                                            # Paths to the two models for binary Junipr.
        quark_model_path:
        quark_config_path:
        gluon_model_path:
        gluon_config_path:

    assess_number_of_jets:          10                        # Number of jet tree to produce in assessing
    Junipr_Dataset:
        padding_size:               80                        # Padded target size for all tensors to be batched to the recurrence (the real size is stored in n_branching)
        padding_value:              0.0                       # The padded-value. To be chosen carefully (otherwise you're gonna remove real data)
        feature_scaling_parameters: [600.0, 0.1, 1.571, 0.01]     # [E_jet, E_sub, R_jet, R_sub] used for scaling values. 1.571 value for R_jet is pi/2
        granularity:                10                        # To bin the values of branches
    lr:                             0.001
    lr_scheduler:                   True
    epoch:                          1
    batch_size:                     1
    initialisation:                 xavier_normal             # Initialisation of weights: xavier_uniform or xavier_normal
    test_frequency:                 1                         # Frequency of test error estimation during traing (for plotting purpose)
    optimiser:
        type:                       adam
        params:                     [0.9, 0.999, 0.00000001]  # Parameters of the optimiser: should be general enough but could be optimised.
        weight_decay:               0.0000001                 # Weight decay for L2 regularisation

    Structure:
        branch_structure:           unique                    # Takes value "unique" or "multiple". Descibes the number of network in the branch mapping
        Recurrent:                                            # Describe the RNN network
            Init:                                             # Parameters of the NN mapping seed-momenta -> initial hidden-state
                input_dimensions:   [4, 10]                   # WARNING, second number must agree with hidden_dimension of Recurrent
                initialisation:     xavier_normal
                nonlinearity:       tanh                      # if one layer, only end_nonlinearity matters
                end_nonlinearity:   tanh                      # needs to be specified as the output is part of the input of RNN
            input_dimensions:       8                         # size of the input data (the input of the RNN is input+hidden)
            hidden_dimensions:      10                        #
            initialisation:         xavier_normal             #
            nonlinearity:           tanh                      #
        JuniprEnd:
            input_dimensions:       [10, 10, 1]               # WARNING: First dimension has to agree with Recurrent/hidden_dimensions
            initialisation:         xavier_normal             #
            nonlinearity:           relu                      #
            end_nonlinearity:       identity                  # identity since loss function used applied a sigmoid itself
        JuniprMother:
            input_dimensions:       [10, 10, 80]              # WARNING: First dimension has to agree with Recurrent/hidden_dimensions/ and third with padding size
            initialisation:         xavier_normal             #
            nonlinearity:           relu                      #
            end_nonlinearity:       identity                  # identity since loss function used applied a log softmax itself
        # Next one for Unique branch
        JuniprBranch:
            input_dimensions:       [14, 10, 40]              # WARNING: First dimension has to agree with Recurrent/hidden_dimensions and third with 4*granularity
            initialisation:         xavier_normal             #
            nonlinearity:           relu                      #
            end_nonlinearity:       identity                  # identity since loss function used applied a the log softmax(es) itself
        # The four next ones for Multiple branch. For each of these, third dimensions needs to agree with granularity
        JuniprBranchZ:
            input_dimensions:       [14, 10, 10]              # WARNING: First dimension has to agree with Recurrent/hidden_dimensions+mother and last with granularity
            initialisation:         xavier_normal             #
            nonlinearity:           relu                      #
            end_nonlinearity:       identity                  # identity since loss function used applied a log softmax itself
        JuniprBranchT:
            input_dimensions:       [15, 10, 10]              # WARNING: First dimension has to agree with Recurrent/hidden_dimensions+mother + 1 and last with granularity
            initialisation:         xavier_normal             #
            nonlinearity:           relu                      #
            end_nonlinearity:       identity                  # identity since loss function used applied a log softmax itself
        JuniprBranchD:
            input_dimensions:       [16, 10, 10]              # WARNING: First dimension has to agree with Recurrent/hidden_dimensions+mother + 2 and last with granularity
            initialisation:         xavier_normal             #
            nonlinearity:           relu                      #
            end_nonlinearity:       identity                  # identity since loss function used applied a log softmax itself
        JuniprBranchP:
            input_dimensions:       [17, 10, 10]              # WARNING: First dimension has to agree with Recurrent/hidden_dimensions+mother +  3 and last with granularity
            initialisation:         xavier_normal             #
            nonlinearity:           relu                      #
            end_nonlinearity:       identity                  # identity since loss function used applied a log softmax itself
# Note: all WARNINGs above are handled in constructor of network, enforcing hidden_dimension, granularity and padding upon them

Multi_model:
    # The list of models to assess in format (model_type, path_to_model, path_to_model_config.yaml, description_string)
    # Model typ is NN or BDT; path_to_model_config.yaml is only necessary for NN; description_string is with "_" separator between words
    # models:                         [[NN, /home/draguet/QGdis/Code/Experiments/store_model/saved_NN_weights.pt, /home/draguet/QGdis/Code/Experiments/store_model/config.yaml, NN_20_unit_hidden_2_epochs], [BDT, /home/draguet/QGdis/Code/Experiments/store_model/saved_model.joblib, /home/draguet/QGdis/Code/Experiments/store_model/config.yaml, BDT_model ]]
    models:                         store_model2                 # name of directory with models and the models_to_load.py loader
    data:
        test_set_type:              regular                   # What sort of test data:
                                    # regular refers to dataset, cs (cross-section based),
                                    # energy (for energy_range specified) or process  (for files in process_list )
        energy_range:               [0, 100]
        process_list:               [None]

UpRootTransformer:
    save_path:                      /data/atlas/atlasdata3/mdraguet/Set3/    # where to store the result
    to_CSV:                         False                                    # Save to CSV
    to_HDF5:                        True                                     # Save to HDF5

GranularUpRootTransformer:
    save_path:                      /data/atlas/atlasdata3/mdraguet/Set4_quark_gluon/    # where to store the result
    save_tables:                    True                                     # Whether to store the intermediary tables of constituents and jets (format define below)
    clean_jets:                     True                                     # Bool to clean the jets
    to_CSV:                         False                                    # Save to CSV
    to_HDF5:                        True                                     # Save to HDF5
    JUNIPR_transform:               True                                     # Do the JUNIPR transform
    JUNIPR_cluster_algo:            cambridge                                # The clustering algorithm to use
    JUNIPR_cluster_radius:          0.5                                      # The radius to use in clustering algorithm
    save_JUNIPR_transform:          True                                     # Save the result into a json file
    cut_train_test:                 True                                     # If true, will separate the json files created into a train and a test one (for each root files fed)
    test_size:                      0.2                                      # If this the case, fraction of the data to be used for the test_size

Train_Test_Separator:
    save_path:                      /data/atlas/atlasdata3/mdraguet/Set3/HF/Separated/
    test_size:                      0.2                                     # Fraction of the whole data to reserve for test
    validation_size:                0.2                                     # Fraction of the whole data to reserve for validation
